Pseudocode or steps:
    1.Compute the entropy for data-set: entropy(s)
    2. For every attribute or feature:
        Calculate entropy for all other value: entropy(a)
        Take average information entropy for current attribute
         Calculate information gain for the current attribute
    3. Pick highest gain attribute
    4. Repeat until we get the tree we desired.

Decision Tree:

If else statement
Search tree and backtracking 
Build optimal decision tree for MONK 1,2,3 dataset  for classification
Bias and variance
Tree for monk1, tree for monk2, tree for monk3 and give accuracy
Pre pruning( stoping at particular height)  and post pruning
Combination: fix the height  max height
Get this rule back (might not)
Divide into training, validation and test set

C5.0, improvement to C4.5 which is improvement on ID3
Advanced pruning 
Soft thresholds

Post pruning -
 Every node remove

Test tree on validation after the split and before the split
And see if there is an improvement in accuracy

Pre pruning :

Threshold - at particular set threshold, 

Height- may be balanced or unbalanced 

Test k on monk1, monk 2, monk3 
Draw graph for validation set with excel
accuracy of the set